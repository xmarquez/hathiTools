---
title: "Topic Modeling using the Hathi Trust Extracted Features Files"
warning: false
format: 
  gfm:
    output-file: "topic_modeling_using_hathi_ef.Rmd"
    output-ext: "Rmd"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.path = "images/topic_modeling_using_hathi_ef-", 
  comment = "#>"
)
```

The Hathi Trust digital library makes available "extracted features" files (files with per-page word count and part of speech information) for millions of digitized volumes from large research libraries. One attraction of these files is that even though they do not contain the full text of each page, page-level "bags of words" are more than sufficient for a vast variety of tasks in the Digital Humanities, from topic modeling to basic word vector models to predictive models. This vignette explains how one can use these files to do basic topic modeling of many volumes.

## An example: A page-level topic model of different editions of Tocqueville's *Democracy in America*

Alexis de Tocqueville's *Democracy in America* is a very influential text. Ever since its original publication in 1835, it has been translated into multiple languages and republished many times. In this vignette, we use the Hathi Trust extracted features files of its many editions to train a page-level topic model that can help visualize how these translations and editions vary.

### Selecting the sample

As a starting point, we can download the IDs of all volumes where Alexis the Tocqueville appears as an author in the 15.6 million volumes indexed by the Hathi Trust [Workset Builder](https://solr2.htrc.illinois.edu/solr-ef/):

```{r setup}
library(hathiTools)
library(tidyverse)
library(jsonlite) # For processing some results

tocqueville <- workset_builder(name = "Alexis de Tocqueville")
tocqueville
```

There are over 400 volumes which list Tocqueville as an author. Many of these are the same, but digitised from different libraries, or different editions of the same text, and some are in different languages (French and English, for example). We can get a glimpse of the variety by downloading the metadata for a sample of 20 volumes:

```{r tocqueville_meta}
set.seed(14)
tocqueville_meta <- get_workset_meta(tocqueville %>%
                                       slice_sample(n = 20))
tocqueville_meta %>%
  select(title, pubDate, publisher, contributor, language) %>%
  rowwise() %>%
  mutate(publisher = jsonlite::fromJSON(publisher) %>% pluck("name") %>% list(),
         contributor = jsonlite::fromJSON(contributor) %>% pluck("name") %>% list()) %>%
  ungroup() %>%
  knitr::kable()

```
(Note that you normally won't want to be using `get_workset_meta()` for more than 100 or so volumes - it will take a long time; best to cache the files locally and then extract their metadata, as explained in `[the example workflow vignette][vignette("example_workflow")]`).

The function `workset_builder()` by default returns a data frame with column for the Hathi Trust id and a column for how many times the `token` key is mentioned; since we didn't include any tokens to search for in our query, the function just returned the number of pages in each volume. If we instead search for the volumes where Tocqueville is an author that *also* mention the word "democracy", the function returns both the volume id and the number of pages that contain the term.

```{r}
tocqueville2 <- workset_builder("democracy", name = "Alexis de Tocqueville")

tocqueville2

```

This is a smaller workset, since it only includes English tokens, and only volumes by Tocqueville that include the word "democracy" (most of them!). It's also possible to get the exact page sequence numbers where the term appears:

```{r}
tocqueville3 <- workset_builder("democracy", name = "Alexis de Tocqueville", volumes_only = FALSE)

tocqueville3

```

We can browse any of these pages interactively on the Hathi Trust website, just to get a sense of what they look like. (Most will be in the public domain; note that the page sequences are zero-indexed, so they are one less than the actual page shown on the Hathi Trust website).

```r
browse_htids(tocqueville3)

```

For our topic modeling exercise, we will focus on volumes of Tocqueville published since 1950 (better OCR!) with "Democracy in America" in the title.

```{r}
tocqueville4 <- workset_builder(name = "Alexis de Tocqueville", pub_date = 1950:2020, title = "Democracy in America")
tocqueville4
```

In principle we can download their full metadata using the function get_workset_meta(), though the SOLR service hasn't been working recently.  (Also, it's a slow download, so use with care if you have a lot of Hathi Trust ids!).

```{r}
tocqueville_meta <- get_workset_meta(tocqueville4)
tocqueville_meta
```

There are `r nrow(tocqueville_meta)` volumes listed here, but many are the same book; sometimes the title has just been entered slightly differently in different cataloguing systems, or republished multiple times, or it's another volume of the same edition. We find 14 distinct bibliographical records with 14 distinct titles which we can investigate:

```{r}
tocqueville_meta %>%
  summarise(n_records = n_distinct(htid),
            n_titles = n_distinct(title))
```

### Downloading and caching the associated Extracted Features files

We first download the "extracted features" files for these texts through the Hathi Trust rsync server. The `rsync_from_hathi()` function attempts to do this automatically (to the `/hathi-ef` directory by default, which will be created if it doesn't exist; this is the default option ). This function requires that your system has access to `rsync`. (In Windows, the easiest way to do this is to install the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install); `rsync` should come already installed in most Mac and Linux machines).

```{r, include = FALSE}

clear_cache(tocqueville4, keep_json = FALSE)

```


```{r}
# The current directory for caching:
getOption("hathiTools.ef.dir")

rsync_from_hathi(tocqueville4)

# Can also be specified explicitly

```

The downloaded files are in JSON format, which is slow to load and parse into data frames. We can cache these files to fast-loading CSVs (or to other formats, including "Feather" or "Parquet" formats from the [arrow][arrow::arrow] package) in the "./hathi-ef" directory using the function `cache_htids()`:

```{r}
cached_files <- cache_htids(tocqueville4)
cached_files
```

The convenience function `find_cached_htids()`, when called with a vector of Hathi Trust IDs or a workset, returns a data frame with the Hathi Trust id, the local file paths of the cached EF files, and whether each file exists or not (i.e., whether it was downloaded successfully and cached).

```{r}
cached_files <- find_cached_htids(tocqueville4)

cached_files
```

These files can now be loaded into R very quickly:

```{r}
tocqueville_ef <- read_cached_htids(tocqueville4)
tocqueville_ef
```


Note that `cache_htids()` automatically caches not only the extracted features file, but also the volume-level and page-level metadata; the function `read_cached_htids()` by default adds both of these forms of metadata to your data frame. But we can also extract the page-level and volume-level metadata separately:

```{r}

tocqueville_ef <- read_cached_htids(tocqueville4, cache_type = "ef")
tocqueville_meta <- read_cached_htids(tocqueville4, cache_type = "meta")
tocqueville_pagemeta <- read_cached_htids(tocqueville4, cache_type = "pagemeta")

tocqueville_ef
tocqueville_meta
tocqueville_pagemeta
```


### Creating a document-term matrix and fitting the model

This large data frame can be converted into a `quanteda::dfm()` document-feature matrix using the `{tidytext}` package (with each page a different document). Here we filter the data frame so that it contains only nouns (POS = NN or NNP or NOUN) in the "body" section of the page, excluding all strings smaller than 3 characters and all pages that end up with fewer than 10 tokens. We will use the results to calculate a per-page topic model.

```{r}
library(tidytext)
library(quanteda)

tocqueville_meta <- tocqueville_meta %>%
  rowwise() %>%
  mutate(contributor = jsonlite::fromJSON(contributor) %>% 
              pluck("name") %>%
              paste(collapse = ", "),
         publisher = jsonlite::fromJSON(publisher) %>% 
              pluck("name") %>%
              paste(collapse = ", "),
         editor = str_remove(contributor, "Tocqueville, Alexis de, 1805-1859.?, ") %>%
           str_extract_all("Goldhammer|Schleifer|Nolla|Mayer|Bender|Commager|Reeve|Bowen|Bradley|Boorstin", 
                           simplify = TRUE) %>%
           paste(collapse = ", "),
         short_publisher = str_extract(publisher, "Knopf|Vintage|Britannica|Schocken |Sever & Francis|Oxford|Modern Library|Liberty Fund|Doubleday|Library of America") %>%
           paste(collapse = ", "),
         edition = str_c(short_publisher, " (", editor, ") ", 
                         ifelse(is.na(enumerationChronology), "", str_to_lower(str_remove(enumerationChronology, 
                                                                                          " ")))) %>%
           str_remove(" \\(\\)"),
         volume = parse_number(enumerationChronology) * 10 %>%
           as.integer()) %>%
  ungroup()

tocqueville_meta %>%
  select(htid, pubDate, edition, volume)
```

```{r}

tocqueville_ef <- tocqueville_ef %>%
  filter(section == "body", !str_detect(token, "[^[:alnum:]]"),
         str_detect(POS, "NN|NOUN"), str_length(token) > 2) %>%
  mutate(text_id = paste(htid, page, sep = "_")) %>%
  group_by(text_id) %>%
  mutate(num_tokens = n()) %>%
  group_by(htid) %>%
  mutate(prop_page = page/max(page)) %>%
  ungroup() %>%
  filter(num_tokens > 10) %>% # This selects pages with at least 10 tokens
  left_join(tocqueville_meta) %>%
  left_join(tocqueville_pagemeta) %>%
  filter(calculatedLanguage == "en")

tocqueville_dfm <- tocqueville_ef %>%
  cast_dfm(text_id, token, count)

docvars(tocqueville_dfm) <- tocqueville_ef %>%
  select(-token, -POS, -count, -section) %>%
  distinct()

tocqueville_dfm
```

We can use this document-feature matrix in a variety of ways. Here I train a page-level topic model using the `{stm}` package, using only the English texts of Tocqueville.

First we subset the document-feature matrix, lowercase its features, remove stopwords, and trim the vocabulary to the top 20,000 features.

```{r}
tocqueville_dfm_eng <- dfm_tolower(tocqueville_dfm)  %>%
  dfm_trim(20000, termfreq_type = "rank") %>%
  dfm_remove(stopwords())

tocqueville_dfm_eng
```

We then fit a structural topic model with 20 topics. This takes a while; normally you'd also do some sensitivity analysis and check whether the model works better with more or fewer topics, but this model is only illustrative (see [this very useful article by Julia Silge](https://juliasilge.com/blog/evaluating-stm/) on training, evaluating, and interpreting topic models). I also don't incorporate any prevalence correlates, but this is trivial to do given that the metadata is incorporated into the document-feature matrix.

```{r}
#| cache: true
library(stm)

if(interactive()) {
  verbose <- TRUE
} else {
  verbose <- FALSE
}

model <- stm(tocqueville_dfm_eng, K = 20, verbose = verbose)

```

We then tidy this model using the tidiers from the `{tidytext}` package.

```{r}
#| message: false
tidy_model <- tidy(model, matrix = "gamma",
                   document_names =  docvars(tocqueville_dfm_eng)$text_id)

docs <- docvars(tocqueville_dfm_eng) %>%
  as_tibble() %>%
  mutate(document = text_id)

tidy_model <- tidy_model %>%
  left_join(docs)

labels <- labelTopics(model)$prob %>%
  t() %>%
  as_tibble(.name_repair = "unique") %>%
  pivot_longer(everything(), names_to = "topic", values_to = "word") %>%
  group_by(topic) %>%
  summarise(label = paste(word, collapse = ",")) %>%
  ungroup() %>%
  mutate(topic = str_remove(topic, fixed("...")) %>%
           as.integer()) %>%
  arrange(topic)

tidy_model <- tidy_model %>%
  left_join(labels)

tidy_model
```

We can now visualize the topic distribution per book:

```{r, fig.width=14, fig.height=12, fig.cap = "Figure 1", cache = FALSE}
library(ggtext)


tidy_model %>%
  mutate(editions = paste(edition, "HT#", htid)) %>%
  ggplot() +
  geom_col(aes(x = page,  y = gamma, fill = label),
           position = "fill",
           width = 1) +
  facet_grid(editions~., switch = 'y') +
  # geom_vline(data = chapters, aes(xintercept = page)) +
  theme_bw() +
    theme(
      panel.spacing.y=unit(0, "lines"),
      strip.text.y.left = element_text(size = 7, angle = 0, hjust = 1),
      legend.position = "bottom",
      axis.text.y = element_blank(),
      axis.title.y = element_blank(),
      axis.ticks.y = element_blank(),
      panel.border = element_blank(),
      strip.background = element_blank(),
      plot.title = element_markdown(),
      panel.grid= element_blank()
      ) +
  scale_x_continuous() +
  scale_fill_discrete(type =  hcl.colors(23, palette = "RdYlBu", rev = TRUE)) +
  labs(title = "Topic distribution in different volumes and editions of translations of Tocqueville's *Democracy in America*",
       x = "page sequence",
       fill = "") +
  guides(fill = guide_legend(ncol = 3))

```

We can see here some broad patterns: volume 2 of *Democracy in America* has more "abstract" topics (about equality, power, virtue, science, etc.) than volume 1; abridgements preserve roughtly the distribution of topics of the non-abridged versions (except for sections at the very end); Schelifer and Nolla's critical edition has facing-page French (which appears as missing pages, excluded from the analysis because the language is not English) and critical apparatus in footnotes (so the distribution of topics looks different); etc. More could be said here, but this is just a starting point!
