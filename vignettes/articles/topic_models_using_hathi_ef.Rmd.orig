---
title: "Topic Modeling using the Hathi Trust Extracted Features Files"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE,
  fig.path = "images/topic-modeling-",
  comment = "#>"
)
```

The Hathi Trust digital library makes available "extracted features" files (files with per-page word count and part of speech information) for millions of digitized volumes from large research libraries. One attraction of these files is that even though they do not contain the full text of each page, page-level "bags of words" are more than sufficient for a vast variety of tasks in the Digital Humanities, from topic modeling to basic word vector models to predictive models. This vignette explains how one can use these files to do basic topic modeling of many volumes.

## An example: A page-level topic model of different editions of Tocqueville's *Democracy in America*

Alexis de Tocqueville's *Democracy in America* is a very influential text. Ever since its original publication in 1835, it has been translated into multiple languages and republished many times. In this vignette, we use the Hathi Trust extracted features files of its many editions to train a page-level topic model that can help visualize how these translations and editions vary.

### Selecting the sample

As a starting point, we can download the IDs of all volumes where Alexis the Tocqueville appears as an author in the 15.6 million volumes indexed by the Hathi Trust [Workset Builder](https://solr2.htrc.illinois.edu/solr-ef/):

```{r setup}
library(hathiTools)
library(tidyverse)

tocqueville <- workset_builder(name = "Alexis de Tocqueville")
tocqueville
```

There are over 400 volumes which list Tocqueville as an author. Many of these are the same, but digitised from different libraries, or different editions of the same text, and some are in different languages (French and English, for example). We can get a glimpse of the variety by downloading the metadata for a sample of 20 volumes:

```{r tocqueville_meta}
set.seed(14)
tocqueville_meta <- get_workset_meta(tocqueville %>%
                                       slice_sample(n = 20))
tocqueville_meta %>%
  select(title, pubDate, imprint, names, language) %>%
  mutate(title = iconv(title, from = "UTF-8", to = "ASCII//TRANSLIT") %>% # Needed so that the table below displays properly
           str_remove_all("`") %>%
           str_remove_all("\\?")) %>%
  knitr::kable()

```

The function `workset_builder()` by default returns a data frame with column for the Hathi Trust id and a column for how many times the `token` key is mentioned; since we didn't include any tokens to search for in our query, the function just returned the number of pages in each volume. If we instead search for the volumes where Tocqueville is an author that *also* mention the word "democracy", the function returns both the volume id and the number of pages that contain the term.

```{r}
tocqueville2 <- workset_builder("democracy", name = "Alexis de Tocqueville")

tocqueville2

```

This is a smaller workset, since it only includes English tokens, and only volumes by Tocqueville that include the word "democracy" (most of them!). It's also possible to get the exact page sequence numbers where the term appears:

```{r}
tocqueville3 <- workset_builder("democracy", name = "Alexis de Tocqueville", volumes_only = FALSE)

tocqueville3

```

We can browse any of these pages interactively on the Hathi Trust website, just to get a sense of what they look like. (Most will be in the public domain; note that the page sequences are zero-indexed, so they are one less than the acual page shown on the Hathi Trust website).

```r
browse_htids(tocqueville3)

```

For our topic modeling exercise, we will focus on volumes of Tocqueville published since 1950 (better OCR!) with "Democracy in America" in the title.

```{r}
tocqueville4 <- workset_builder(name = "Alexis de Tocqueville", pub_date = 1950:2020, title = "Democracy in America")
tocqueville4
```

We can download their full metadata as follows (it's a slow download, so use with care if you have a lot of Hathi Trust ids!):

```{r}
tocqueville_meta <- get_workset_meta(tocqueville4)
tocqueville_meta
```

There are `r nrow(tocqueville_meta)` volumes listed here, but many are the same book; sometimes the title has just been entered slightly differently in different cataloguing systems, or republished multiple times, or it's another volume of the same edition. We find 14 distinct bibliographical records with 14 distinct titles which we can investigate:

```{r}
tocqueville_meta %>%
  summarise(n_records = n_distinct(hathitrustRecordNumber),
            n_titles = n_distinct(title))
```

### Downloading and caching the associated Extracted Features files

We first download the "extracted features" files for these texts through the Hathi Trust rsync server. The `rsync_from_hathi()` function attempts to do this automatically (to the `/hathi-ef` directory by default, which will be created if it doesn't exist). This function requires that your system has access to `rsync`..


```{r}
rsync_from_hathi(tocqueville4)
```

The downloaded files are in JSON format, which is slow to load and parse into data frames. We can cache these files to fast-loading CSVs in the "./hathi-ef" directory using the function `cache_htids()`:

```{r}
cache_htids(tocqueville4)
```

The convenience function `find_cached_htids()`, when called with a vector of Hathi Trust IDs or a workset, returns a data frame with the Hathi Trust id, the local file paths of the cached files, and whether each file exists or not (i.e., whether it was downloaded successfully and cached).

```{r}
cached_files <- find_cached_htids(tocqueville4)

cached_files
```

The reason for an `exists` column is that sometimes extracted features files are not downloaded successfully -- they are not available in the Hathi Trust rsync server for whatever reason -- and so need to be excluded from further analysis (by filtering the returned dataset via `cached_files %>% filter(exists)`, for examples). In any case these files can now be loaded into R very quickly:

```{r, message = FALSE}
tocqueville_ef <- cached_files %>%
  filter(exists) %>%
  pull(local_loc) %>%
  set_names(cached_files$htid[cached_files$exists]) %>%
  map_df(vroom::vroom, .id = "htid")

tocqueville_ef
```

### Creating a document-term matrix and fitting the model

This large data frame can be converted into a `quanteda::dfm()` document-feature matrix using the `{tidytext}` package (with each page a different document). Here we filter the data frame so that it contains only nouns (POS = NN or NNP or NOUN) in the "body" section of the page, excluding all strings smaller than 3 characters and all pages that end up with fewer than 10 tokens. We will use the results to calculate a per-page topic model.

```{r}
library(tidytext)
library(quanteda)

tocqueville_meta$editions <- c("Reeve, Bowen, Bradley", "Nolla-Shleifer", "Reeve, Bowen, Bradley",
                      "Reeve, Bowen, Bradley", "Reeve, Bowen, Bradley", "Reeve, Bowen, Bradley",
                      "Reeve, Bowen, Bradley", "Schocken books Mill appraisal", "Sinclair-Probst dramatizations",
                      "Reeve, Bowen, Bradley", "Reeve, Bowen, Bradley", "Reeve, Bowen, Bradley",
                      "Goldhammer", "Bender abridgement of Reeve, Bowen, Bradley", "Lawrence",
                      "Reeve", "Schocken books Mill appraisal", "Reeve, Bowen, Bradley", "Reeve, Bowen, Bradley",
                      "Mayer-Lawrence", "Reeve", "Commager abridg. Reeve", "Commager abridg. Reeve")

tocqueville_meta <- tocqueville_meta %>%
  mutate(editions = paste(editions, enumerationChronology, pubDate))

tocqueville_ef <- tocqueville_ef %>%
  filter(section == "body", !str_detect(token, "[^[:alnum:]]"),
         str_detect(POS, "NN|NOUN"), str_length(token) > 2) %>%
  mutate(text_id = paste(htid, page, sep = "_")) %>%
  group_by(text_id) %>%
  mutate(num_tokens = n()) %>%
  group_by(htid) %>%
  mutate(prop_page = page/max(page)) %>%
  ungroup() %>%
  filter(num_tokens > 10) %>% # This selects pages with at least 10 tokens
  left_join(tocqueville_meta %>% # This adds the metadata
              rename(htid = volumeIdentifier))

tocqueville_dfm <- tocqueville_ef %>%
  cast_dfm(text_id, token, count)

docvars(tocqueville_dfm) <- tocqueville_ef %>%
  select(-token, -POS, -count) %>%
  distinct()

tocqueville_dfm
```

We can use this document-feature matrix in a variety of ways. Here I train a page-level topic model using the `{stm}` package, using only the English texts of Tocqueville.

First we subset the document-feature matrix, lowercase its features, remove stopwords, and trim the vocabulary to the top 20,000 features.

```{r}
tocqueville_dfm_eng <- dfm_subset(tocqueville_dfm, subset = language == "eng") %>%
  dfm_tolower() %>%
  dfm_trim(20000, termfreq_type = "rank") %>%
  dfm_remove(stopwords())

tocqueville_dfm_eng
```

We then fit a structural topic model with 20 topics. This takes a while; normally you'd also do some sensitivity analysis and check whether the model works better with more or fewer topics, but this model is only illustrative (see [this very useful article by Julia Silge](https://juliasilge.com/blog/evaluating-stm/) on training, evaluating, and interpreting topic models). I also don't incorporate any prevalence correlates, but this is trivial to do given that the metadata is incorporated into the document-feature matrix.


```{r}
library(stm)

model <- stm(tocqueville_dfm_eng, K = 20, verbose = FALSE)
# model <- stm(tocqueville_dfm_eng, K = 20, verbose = FALSE, prevalence = ~editions) # With prevalence covariates

```

We then tidy this model using the tidiers from the `{tidytext}` package.

```{r}
tidy_model <- tidy(model, matrix = "gamma",
                   document_names =  docvars(tocqueville_dfm_eng)$text_id)

docs <- docvars(tocqueville_dfm_eng) %>%
  as_tibble() %>%
  mutate(document = text_id)

tidy_model <- tidy_model %>%
  left_join(docs)

labels <- labelTopics(model)$prob %>%
  t() %>%
  as_tibble(.name_repair = "unique") %>%
  pivot_longer(everything(), names_to = "topic", values_to = "word") %>%
  group_by(topic) %>%
  summarise(label = paste(word, collapse = ",")) %>%
  ungroup() %>%
  mutate(topic = str_remove(topic, fixed("...")) %>%
           as.integer()) %>%
  arrange(topic)

tidy_model <- tidy_model %>%
  left_join(labels)

tidy_model
```

We can now visualize the topic distribution per book:

```{r, fig.width=14, fig.height=12, fig.cap = "Figure 1", cache = FALSE}
library(ggtext)


tidy_model %>%
  mutate(editions = str_replace_all(editions, "NA", "1 vol. edition") %>%
           str_replace_all("V\\.[ ]?", "v.") %>%
           str_replace_all("v\\. ", "v."),
         editions = paste(editions, "HT#", hathitrustRecordNumber)) %>%
  ggplot() +
  geom_col(aes(x = page,  y = gamma, fill = label),
           position = "fill",
           width = 1) +
  facet_grid(editions~., switch = 'y') +
  # geom_vline(data = chapters, aes(xintercept = page)) +
  theme_bw() +
    theme(
      panel.spacing.y=unit(0, "lines"),
      strip.text.y.left = element_text(size = 7, angle = 0, hjust = 1),
      legend.position = "bottom",
      axis.text.y = element_blank(),
      axis.title.y = element_blank(),
      axis.ticks.y = element_blank(),
      panel.border = element_blank(),
      strip.background = element_blank(),
      plot.title = element_markdown(),
      panel.grid= element_blank()
      ) +
  scale_x_continuous() +
  scale_fill_discrete(type =  hcl.colors(23, palette = "RdYlBu", rev = TRUE)) +
  labs(title = "Topic distribution in different volumes and editions of translations of Tocqueville's *Democracy in America*",
       x = "page sequence",
       fill = "") +
  guides(fill = guide_legend(ncol = 3))

```

We can see here some broad patterns: volume 2 of *Democracy in America* has more "abstract" topics (about equality, power, virtue, science, etc.) than volume 1; abridgements preserve roughtly the distribution of topics of the non-abridged versions (except for sections at the very end); the Nolla-Schleifer critical edition of the book clearly has facing-page French text, which accounts for the pattern of missing pages, and a lot of critical apparatus, which increases the proportion of certain topics on the page not common in other editions; etc. More could be said here, but this is just a starting point!
