---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# hathiTools

<!-- badges: start -->
[![R-CMD-check](https://github.com/xmarquez/hathiTools/workflows/R-CMD-check/badge.svg)](https://github.com/xmarquez/hathiTools/actions)
<!-- badges: end -->

This package allows you to interact with various free data resources made available by the Hathi Trust digital library, including the Hathi Trust [Bookworm](https://bookworm.htrc.illinois.edu/develop/), a tool similar to the [Google ngram viewer](https://books.google.com/ngrams) and the Hathi Trust [Workset Builder 2.0](https://solr2.htrc.illinois.edu/solr-ef/). You can also download and process the [Hathi Trust Extracted Features](https://analytics.hathitrust.org/datasets) files on which the Bookworm viewer is based. The Hathi Trust collection contains over 17 million digitised books, including many of those originally digitised by Google for its Google Books project. 

## Installation

This package is not yet on CRAN. Install from GitHub as follows:

``` r
remotes::install("xmarquez/hathiTools")
```

## Downloading word frequencies

The simplest task to use the package for is to download word frequencies from the Hathi Trust [Bookworm](https://bookworm.htrc.illinois.edu/develop/):

```{r example}
library(hathiTools)
library(tidyverse)

result <- query_bookworm(word = c("democracy", "monarchy"), lims = c(1760, 2000), counttype = c("WordsPerMillion", "TextPercent"))

result

result %>%
  group_by(word, counttype) %>%
  mutate(rolling_avg = slider::slide_dbl(value, mean, .before = 10, .after = 10)) %>%
  ggplot(aes(x = date_year, color = word)) +
  geom_line(aes(y = value), alpha = 0.3) +
  geom_line(aes(x = date_year, y = rolling_avg)) +
  facet_wrap(~counttype) +
  labs(x = "Year", y = "", subtitle = "10 year rolling average, books published between 1760-2000",
       title = "Frequency of 'democracy' and 'monarchy' in the HathiTrust corpus") +
  theme_bw()

```

It is also possible to do more complex queries, for example to look at the relative frequency of a term across book classifications from the Library of Congress system:

```{r example2}
result2 <- query_bookworm(word = "democracy", groups = c("date_year", "class"),
                          lims = c(1900,2000))

result2

result2 %>%
  ggplot(aes(x = date_year, y = fct_reorder(str_trunc(class, 40), value))) +
  geom_tile(aes(fill = value)) +
  facet_wrap(~word) +
  scale_fill_gradient2() +
  theme_bw() +
  labs(y = "", x = "Year", title = "Frequency of 'democracy' \nacross library of congress classifications",
       fill = "Words per million") +
  theme(legend.position = "bottom")
  
```

Or across literary forms:

```{r example3}
result3 <- query_bookworm(word = "democracy", groups = c("date_year", "literary_form"),
                          lims = c(1900,2000))

result3 %>%
  ggplot(aes(x = date_year, y = fct_reorder(str_trunc(literary_form, 40), value))) +
  geom_tile(aes(fill = value)) +
  facet_wrap(~word) +
  scale_fill_gradient2() +
  theme_bw() +
  labs(y = "", x = "Year", title = "Frequency of 'democracy' \nacross literary forms",
       fill = "Words per million") +
  theme(legend.position = "bottom")
```

It is also possible to further limit the query to, e.g., books published in a particular language. For example, this gives the number of English-language texts that use the word "democracy" per year from 1760-2000. 

```{r example4}
result4 <- query_bookworm(word = c("democracy"), lims = c(1760, 2000), counttype = c("TotalTexts"), language = "English")

result4 
```


One can use `method = "returnPossibleFields"` to return the fields available for grouping:

```{r example5}
result5 <- query_bookworm(word = "", method = "returnPossibleFields")

result5
```

We can also get a sample of the book titles and links for a particular year. For example, suppose we're interested in why so many books in the category "Education" mention the word "democracy" in 1941, as appears in the second graph above. This query pulls the first 100 books in the catalog for 1941 in the category "education":

```{r example6}

result2 %>% filter(class == "Education", value == max(value))

result6 <- query_bookworm(word = "democracy", groups = "date_year",
                          date_year = "1941", class = "Education", method = "search_results")

result6 
```

We can download the Extracted Features file associated with any of these HathiTrust IDs:

```{r example7}

tmp <- tempdir() 

result6$htid[2] %>%
  download_hathi_ef(dir = tmp)

extracted_features <- get_hathi_counts(result6$htid[2], dir = tmp)

extracted_features
```

And we can extract the metadata:

```{r example8}
meta <- get_hathi_meta(result6$htid[2], dir = tmp)

meta
```
We can also get the metadata for all of these books at the same time:

```{r}
meta <- get_workset_meta(result6$htid, metadata_dir = tmp)

meta
```
And one can browse interactively these titles on the Hathi Trust website:

```r
browse_htids(result6)

```

One can get info about the corpus itself by using `counttype = "TotalWords"` or `counttype = "TotalTexts"` and omitting the word key.

```{r example9, warning = FALSE}
result7 <- query_bookworm(counttype = c("TotalTexts"), groups = c("date_year", "language"),
                          lims = c(1500,2000))

result7 %>%
  summarise(value = sum(value))

library(ggrepel)

result7 %>%
  mutate(language = fct_lump_n(language, 10, w = value)) %>%
  group_by(date_year, language) %>%
  summarise(value = sum(value)) %>%
  group_by(language) %>%
  mutate(label = ifelse(date_year == max(date_year), as.character(language), NA_character_)) %>%
  group_by(language) %>%
  mutate(rolling_avg = slider::slide_dbl(value, mean, .before = 10, .after = 10)) %>%
  ggplot() +
  geom_line(aes(x = date_year, y = rolling_avg, color = language), show.legend = FALSE) +
  geom_line(aes(x = date_year, y = value, color = language), show.legend = FALSE, alpha = 0.3) +
  geom_text_repel(aes(x = date_year, y = value, label = label, color = language), show.legend = FALSE) +
  scale_y_log10() +
  theme_bw() +
  labs(title = "Total texts per language in the HathiTrust bookworm", subtitle = "Log scale. Less common languages grouped as 'other'. 10 year rolling average.", x = "Year", y = "")
```

Note that the accessible Hathi Trust Bookworm database is the 2016 version.

It is also possible to build a workset of Hathi Trust IDs for further analysis. Here, for example, we find all the volumes that contain the term "democracy" and have genre strings that include "dictionary" and "biography".

```{r}
result8 <- workset_builder(token = "democracy", genre = c("dictionary", "biography"))

result8 
```

Here's the metadata for the first six of these results.

```{r}
meta <- get_workset_meta(head(result8), metadata_dir = tmp) 

meta
```

One can also turn the workset into a list of htids for downloading via rsync:

```{r}
tmp <- tempfile()

htid_to_rsync(result8$htid, tmp)
```

There's a convenience function that will attempt to do this for you in one command.

```{r, eval = FALSE}
rsync_from_hathi(head(result8$htid))
```

(This requires having rsync installed; see the vignette on using worksets in this website).

It is also possible to download the big "[hathifile](https://www.hathitrust.org/hathifiles)" to get basic metadata for ALL of the texts in the Hathi Trust digital library; this is useful for selecting random samples.

## Credits

This package includes some code from the [hathidy](https://github.com/HumanitiesDataAnalysis/hathidy) and [edinburgh](https://github.com/bmschmidt/edinburgh/) repos by @bmschmidt.
